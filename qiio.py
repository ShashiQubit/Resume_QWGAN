# -*- coding: utf-8 -*-
"""Qiio.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cQwhloqA1RMZ-n09BWOUD63nVT5qLPF9
"""

# hide tensorflow warnings on GPU execution (we will use CPU)

from silence_tensorflow import silence_tensorflow

silence_tensorflow()


# disable GPU usage

import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""

import matplotlib.pyplot as plt
import numpy as np
from scipy.stats import norm, wasserstein_distance, probplot
from scipy.special import lambertw
from statsmodels.graphics import tsaplots
import statsmodels.api as sm
import time
import cirq, random, sympy
import tensorflow_quantum as tfq
import tensorflow as tf
from cirq.contrib.svg import SVGCircuit
from cirq.circuits import InsertStrategy


import pandas as pd



# =============================
# Load your AWS series (replace with your actual file)
# =============================
data = pd.read_csv("iio.csv")  # file must contain a 'value' column
data['value'] = pd.to_numeric(data['value'], errors='coerce')
data.dropna(subset=['value'], inplace=True)
start_date = '2013-10-11'
data = data[data['timestamp'] >= start_date].copy()
series = data['value'].values.astype(float)

# =============================
# Inspect raw
# =============================

# =============================
# Robust scaling to [-1,1]
# =============================
p_low, p_high = np.percentile(series, [0.5, 99.5])
clipped = np.clip(series, p_low, p_high)
scaled = (clipped - p_low) / (p_high - p_low) * 2 - 1
scaled  = tf.convert_to_tensor(scaled)
#print("Scaled series range:", scaled.min(), scaled.max())

class qGAN(tf.keras.Model):

    def __init__(self, num_epochs, batch_size, window_length, n_critic, gp, num_layers, num_qubits):
        super(qGAN, self).__init__()

        # classical hyperparameters
        self.num_epochs = num_epochs
        self.batch_size = batch_size
        self.window_length = window_length
        self.n_critic = n_critic
        self.gp = gp

        # quantum hyperparameters
        # one layer corresponds to a rotation and an entangling layer together
        self.num_layers = num_layers
        self.num_qubits = num_qubits

        # quantum circuit settings
        self.qubits = cirq.GridQubit.rect(1, num_qubits)
        # create the set of Pauli strings to measure -> {X1, Z1, X2, Z2, etc}
        # X1 means we measure the first qubit only with X, Z1 the first qubit only with Z and so on...
        self.measurements = []
        for qubit in self.qubits:
            self.measurements.append(cirq.X(qubit))
            self.measurements.append(cirq.Z(qubit))

        # number of parameters of the PQC and re-uploading layers
        self.num_params = self.count_params()

        # define the trainable parameters of the PQC main and re-uploading layers (trainable)
        self.params_pqc = [sympy.Symbol(f'theta{i}') for i in range(self.num_params)]

        # define the classical critic network (CNN)
        self.critic = self.define_critic_model(window_length)
        # define the quantum generator network (PQC)
        self.generator = self.define_generator_model()

        # monitoring purposes
        # average critic and generator losses for each epoch
        self.critic_loss_avg = []
        self.generator_loss_avg = []
        # Earth's mover distance (EMD) for each epoch
        self.emd_avg = []
        # stylized facts RMSEs for each epoch
        self.acf_avg = []
        self.vol_avg = []
        self.lev_avg = []

    ####################################################################################
    #
    # count the parameters of the quantum circuit
    #
    ####################################################################################
    def count_params(self):

        # rotation layer with Rx, Ry, Rz has 3N parameters, where N is the number of qubits
        # the entangling layer is not parameterized
        num_params_pqc = 3*self.num_qubits*self.num_layers

        # also, count the parameters of the re-uploading layer that is sandwiched between rotation-entangling layers
        # there is one re-uploading layer after each rotation-entangling layer with a parameterized Rx gate,
        # so the number of parameters is equal to the number of qubits for each re-uploading layer
        num_params_upload = self.num_layers*self.num_qubits

        # the last layer of the PQC is a rotation layer
        num_params_pqc += 3*self.num_qubits

        return num_params_pqc+num_params_upload

    ####################################################################################
    #
    # the classical critic model as a convolutional network
    #
    ####################################################################################
    def define_critic_model(self, window_length):
        model = tf.keras.Sequential()
        model.add(tf.keras.layers.Conv1D(filters=64, kernel_size=10, strides=1, input_shape=(window_length, 1), padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))

        model.add(tf.keras.layers.Conv1D(filters=128, kernel_size=10, strides=1, padding='same'))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
    
        model.add(tf.keras.layers.Flatten())
    
        model.add(tf.keras.layers.Dense(32, dtype=tf.float64))
        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))
        model.add(tf.keras.layers.Dropout(0.2))
    
        model.add(tf.keras.layers.Dense(1, dtype=tf.float64))

        return model

    ####################################################################################
    #
    # the encoding layer: resolve the parameters by uniform noise values,
    # used to prepare the initial state for the generator circuit
    #
    ####################################################################################
    def encoding_layer(self, noise_params):

        return cirq.Circuit(cirq.Rx(rads=noise_params[i])(self.qubits[i]) for i in range(self.num_qubits))

    ####################################################################################
    #
    # the quantum generator as a PQC with All-to-all topology for the entangling layer
    #
    ####################################################################################
    def define_generator_circuit(self):

        # cirq circuit
        pqc = cirq.Circuit()

        # index for the parameter tensor of the PQC main and re-uploading layers
        idx = 0

        for layer in range(self.num_layers):
            ###############################################################
            #
            # single-qubit rotation layer
            #
            ###############################################################
            for qubit in self.qubits:
                pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Ry(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Rz(rads=self.params_pqc[idx])(qubit))
                idx += 1

            ###############################################################
            #
            # entangling layer (not parameterized)
            #
            ###############################################################
            for qubit1 in range(self.num_qubits):
                for qubit2 in range(qubit1+1, self.num_qubits):
                    pqc.append(cirq.CZ(self.qubits[qubit1], self.qubits[qubit2]), strategy=InsertStrategy.NEW)

            ###############################################################
            #
            # re-uploading layer with Rx rotation
            # (set the strategy for better readability and understanding)
            #
            ###############################################################
            for i, qubit in enumerate(self.qubits):
                if i == 0:
                    pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit), strategy=InsertStrategy.NEW)
                else:
                    pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit), strategy=InsertStrategy.INLINE)
                idx += 1

        #####################################################################
        #
        # single-qubit rotation layer as the last layer before measurement
        #
        #####################################################################
        for qubit in self.qubits:
                pqc.append(cirq.Rx(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Ry(rads=self.params_pqc[idx])(qubit))
                idx += 1
                pqc.append(cirq.Rz(rads=self.params_pqc[idx])(qubit))
                idx += 1

        return pqc

    ####################################################################################
    #
    # the quantum generator model
    #
    ####################################################################################
    def define_generator_model(self):
        # model input
        q_data_input = tf.keras.Input(shape=(), dtype=tf.dtypes.string)
        # define the tensorflow quantum layer (trainable)
        generator = tfq.layers.PQC(self.define_generator_circuit(), self.measurements, repetitions=1000)
        generator_output = generator(q_data_input)
        # tensorflow model
        model = tf.keras.Model(inputs=q_data_input, outputs=generator_output)

        return model

    #############################################################################
    #
    # compile model with given optimizers for critic and generator networks
    #
    #############################################################################
    def compile_QGAN(self, c_optimizer, g_optimizer):
        super(qGAN, self).compile()
        self.c_optimizer = c_optimizer
        self.g_optimizer = g_optimizer

    def train_qgan(self, gan_data, original_data, preprocessed_data, num_elements):
        """
        Parameters:
         - gan_data is the preprocessed dataset with windows for qGAN training
         - original_data is the original S&P 500 log-returns for evaluation of RMSEs (monitoring purposes)
         - preprocessed_data is the preprocessed log-returns without the last normalization step and without windows
          (for reversing the process of generated samples using the mean and std and evaluating the RMSEs)
        """
        last_epoch = 1500 
        for epoch in range(self.num_epochs):
            print(f'Processing epoch {epoch+1}/{self.num_epochs}')
            ################################################################
            #
            # Train the critic for n_critic iterations
            # Process 'batch_size' samples in each iteration
            #
            ################################################################
            # critic loss for 'n_critic' iterations
            critic_t_sum = 0
            for t in range(self.n_critic):
                # record the gradients
                with tf.GradientTape() as critic_tape:
                    # critic loss for 'batch_size' samples
                    critic_sum = 0
                    for i in range(self.batch_size):
                        ###########################################
                        #
                        # Sample real data and a latent variable
                        #
                        ###########################################
                        # shuffle the dataset
                        shuffled_data = gan_data.shuffle(buffer_size=num_elements)
                        # take a single random element from the shuffled dataset
                        random_element = shuffled_data.take(1)
                        # iterate over the random_element dataset to access the value
                        for element in random_element:
                            # access the value of the random element as a tensor
                            real_sample = element
                        # reshape the real sample for compatibility with the first layer of the critic
                        real_sample = tf.reshape(real_sample, (1, self.window_length))

                        ##################################################
                        #
                        # Get the state prepared by the encoding circuit
                        #
                        ##################################################
                        # generate noise parameters for the encoding layer
                        noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                        generator_input_state = self.encoding_layer(noise_values)
                        # convert to tensorflow quantum tensor
                        generator_input = tfq.convert_to_tensor([generator_input_state])
                        # get the fake sample as the expectations of the quantum circuit
                        generated_sample = self.generator(generator_input)
                        generated_sample = tf.cast(generated_sample, dtype=tf.float64)

                        # calculate the critic scores for real and fake samples
                        real_score = self.critic(real_sample)
                        fake_score = self.critic(generated_sample)

                        # compute the gradient penalty
                        gradient_penalty = self.compute_gradient_penalty(real_sample, generated_sample)

                        # calculate the Wasserstein distance loss with gradient penalty
                        critic_loss = fake_score - real_score + self.gp * gradient_penalty
                        # accumulate the critic loss for the sample
                        critic_sum += critic_loss

                    # compute the gradients of critic and apply them
                    critic_gradients = critic_tape.gradient(critic_sum/self.batch_size, self.critic.trainable_variables)
                    self.c_optimizer.apply_gradients(zip(critic_gradients, self.critic.trainable_variables))

                    # accumulate the average critic loss for all samples in this 't' iteration
                    critic_t_sum += critic_sum/self.batch_size

            # average critic loss for this epoch of WGAN training
            self.critic_loss_avg.append(critic_t_sum/self.n_critic)

            ################################################################
            #
            # Train generator for one iteration
            #
            ################################################################
            # sample a batch of input states using the encoding layer
            input_circuits_batch = []
            for _ in range(self.batch_size):
                noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                input_circuits_batch.append(self.encoding_layer(noise_values))

            # convert to tensorflow quantum tensor
            generator_inputs = tfq.convert_to_tensor(input_circuits_batch)

            with tf.GradientTape() as gen_tape:
                # generate fake samples using the generator
                generated_samples = self.generator(generator_inputs)
                generated_samples = tf.cast(generated_samples, dtype=tf.float64)
                # calculate the critic scores for fake samples
                fake_scores = self.critic(generated_samples)
                # calculate the generator loss
                generator_loss = -tf.reduce_mean(fake_scores)

            # compute the gradients of generator and apply them
            generator_gradients = gen_tape.gradient(generator_loss, self.generator.trainable_variables)
            self.g_optimizer.apply_gradients(zip(generator_gradients, self.generator.trainable_variables))

            # average generator loss for this epoch
            self.generator_loss_avg.append(generator_loss)

            ########################################################################################################
            #
            # Calculate the stylized facts RMSEs and the EMD for real and fake data
            #
            # Fake data has shape (num_samples x window_length), with num_samples = original_length / window_length
            # in order to get a time series close to the length of the original
            #
            ########################################################################################################
            # generate noise
            num_samples = len(original_data) // self.window_length
            input_circuits_batch = []
            for _ in range(num_samples):
                noise_values = np.random.uniform(0, 2 * np.pi, size=self.num_qubits)
                input_circuits_batch.append(self.encoding_layer(noise_values))

            # convert to tensorflow quantum tensor
            generator_inputs = tfq.convert_to_tensor(input_circuits_batch)
            # generate fake samples using the generator
            batch_generated = self.generator(generator_inputs)
            # concatenate all time series data into one
            generated_data = tf.reshape(batch_generated, shape=(num_samples*self.window_length,))
            generated_data = tf.cast(generated_data, dtype=tf.float64)
            # rescale
            generated_data = generated_data
            # reverse the preprocessing on generated sample
            original_norm = generated_data
            fake_original = generated_data
            # calculate the temporal metrics for monitoring the training process
            corr_rmse, volatility_rmse, lev_rmse, emd = self.stylized_facts(original_data, fake_original)
            # store the EMD and RMSEs of stylized facts
            self.acf_avg.append(corr_rmse)
            self.vol_avg.append(volatility_rmse)
            self.lev_avg.append(lev_rmse)
            self.emd_avg.append(emd)

            # checkpoint saving
        
            if (epoch + 1) % 20 == 0:
                save_epoch = last_epoch + (epoch + 1)
                self.generator.save_weights(f"checkpoints/generator_iio_epoch_{save_epoch}.weights.h5")
                self.critic.save_weights(f"checkpoints/critic_epoch_iio_{save_epoch}.weights.h5")

            # print progress every 100 epochs
            if epoch % 100 == 0 or epoch+1 == 3000:
                print(f'\nEpoch {epoch+1} completed')
                print(f'Critic loss (average): {self.critic_loss_avg[epoch][-1][0]}')
                print(f'Generator loss (average): {self.generator_loss_avg[epoch]}')
                print(f'\nEMD (average): {self.emd_avg[epoch]}')
                print(f'ACF RMSE (average): {self.acf_avg[epoch]}')
                print(f'VOLATILITY RMSE (average): {self.vol_avg[epoch]}')
                print(f'LEVERAGE RMSE (average): {self.lev_avg[epoch]}\n')
                print('Min-Max values of original log-returns: ', tf.reduce_min(original_data).numpy(), tf.reduce_max(original_data).numpy())
                print('Min-Max values of generated log-returns (for all batches): ', tf.reduce_min(fake_original).numpy(), tf.reduce_max(fake_original).numpy())
                print('Min-Max values after Lambert: ', tf.reduce_min(original_norm).numpy(), tf.reduce_max(original_norm).numpy())
                print()

    ###########################################################
    #
    # Sample a random number epsilon ~ U[0,1]
    # Create a convex combination of real and generated sample
    # Compute the gradient penalty for the critic network
    #
    ###########################################################
    def compute_gradient_penalty(self, real_sample, generated_sample):
        epsilon = tf.random.uniform((), dtype=tf.float64)
        interpolated_sample = epsilon * real_sample + (1 - epsilon) * generated_sample

        with tf.GradientTape() as tape:
            tape.watch(interpolated_sample)
            scores = self.critic(interpolated_sample)

        gradients = tape.gradient(scores, interpolated_sample)
        gradients_norm = tf.norm(gradients)
        gradient_penalty = (gradients_norm - 1)**2

        return gradient_penalty

    def stylized_facts(self, original_data, fake_original):
        """
        - Calculate the RMSEs of the stylized facts between the original S&P 500 log-returns and
          generated time series

        - Evaluate the EMD between real and generated samples
        """

        ################################################
        #
        # stylized facts for fake samples
        #
        ################################################
        # compute acf for maximum lags = 18
        acf_values = sm.tsa.acf(fake_original, nlags=18)
        # exclude zero lag
        acf_values_generated = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 18
        acf_abs_values = sm.tsa.acf(tf.abs(fake_original), nlags=18)
        # exclude zero lag
        acf_abs_values_generated = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 18
        lev = []
        for lag in range(1, 19):
            # slice the tensors to get the appropriate lagged sequences
            r_t = fake_original[:-lag]
            squared_lag_r = tf.square(tf.abs(fake_original[lag:]))

            # calculate the leverage effect
            # calculate the correlation coefficient
            correlation_matrix = np.corrcoef(r_t, squared_lag_r)
            lev.append(correlation_matrix[0, 1])

        leverage_generated = tf.convert_to_tensor(lev)

        ################################################
        #
        # stylized facts for real samples
        #
        ################################################

        # compute acf for maximum lags = 18
        acf_values = sm.tsa.acf(original_data, nlags=18)
        # exclude zero lag
        acf_values_original = tf.convert_to_tensor(acf_values[1:])

        # compute absolute acf (volatility clustering) for maximum lags = 18
        acf_abs_values = sm.tsa.acf(tf.abs(original_data), nlags=18)
        # exclude zero lag
        acf_abs_values_original = tf.convert_to_tensor(acf_abs_values[1:])

        # compute leverage effect for maximum lags = 18
        lev = []
        for lag in range(1, 19):
            # slice the tensors to get the appropriate lagged sequences
            r_t = original_data[:-lag]
            squared_lag_r = tf.square(tf.abs(original_data[lag:]))

            # calculate the leverage effect
            # calculate the correlation coefficient
            correlation_matrix = np.corrcoef(r_t, squared_lag_r)
            lev.append(correlation_matrix[0, 1])

        leverage_original = tf.convert_to_tensor(lev)

        # calculate average RMSEs of stylized facts
        # autocorrelations
        rmse_acf = tf.sqrt(tf.reduce_mean((acf_values_original-acf_values_generated)**2))
        # volatility clustering
        rmse_vol = tf.sqrt(tf.reduce_mean((acf_abs_values_original-acf_abs_values_generated)**2))
        # leverage effect
        rmse_lev = tf.sqrt(tf.reduce_mean((leverage_original-leverage_generated)**2))

        ####################################################################################
        #
        # compute the Earth's mover distance (EMD)
        #
        ####################################################################################

        bin_edges = np.linspace(-1, 1, num=50)
        empirical_real, _ = np.histogram(original_data, bins=bin_edges, density=True)
        empirical_fake, _ = np.histogram(fake_original, bins=bin_edges, density=True)
        # normalize safely
        if empirical_real.sum() > 0: empirical_real /= empirical_real.sum()
        if empirical_fake.sum() > 0: empirical_fake /= empirical_fake.sum()
        emd = wasserstein_distance(empirical_real, empirical_fake)

        return rmse_acf, rmse_vol, rmse_lev, emd

def rolling_window(data, m, s):
    return tf.map_fn(lambda i: data[i:i+m], tf.range(0, len(data) - m + 1, s), dtype=tf.float64)

def inverse_scale(scaled, p_low, p_high):
    return 0.5 * (scaled + 1.0) * (p_high - p_low) + p_low

##################################################################
#
# Hyperparameters
#
##################################################################
WINDOW_SIZE = 20  # this must be equal to the number of Pauli strings to measure
NUM_QUBITS = 10  # number of qubits
NUM_LAYERS = 7  # number of layers for the PQC

# training hyperparameters
EPOCHS = 1500 #3000
BATCH_SIZE = 10

n_critic = 2 # number of iterations for the critic per epoch
LAMBDA = 10  # gradient penalty strength

# instantiate the QGAN model object
qgan = qGAN(EPOCHS, BATCH_SIZE, WINDOW_SIZE, n_critic, LAMBDA, NUM_LAYERS, NUM_QUBITS)

# set the optimizers
c_optimizer = tf.keras.optimizers.Adam()
g_optimizer = tf.keras.optimizers.Adam()
qgan.compile_QGAN(c_optimizer, g_optimizer)

# load last saved checkpoint weights
qgan.generator.load_weights("checkpoints/generator_iio_epoch_1500.weights.h5")
qgan.critic.load_weights("checkpoints/critic_epoch_iio_1500.weights.h5")

##################################################################################
#
# Data pre-processing
#
##################################################################################
# apply rolling window in transformed normalized log-returns with stride s=5
gan_data_tf = rolling_window(scaled, WINDOW_SIZE, 5)
# create TensorFlow datasets
gan_data = tf.data.Dataset.from_tensor_slices(gan_data_tf)
# get the number of elements in the dataset
num_elements = gan_data.cardinality().numpy()

# train the QWGAN
print('Training started...')
print('Number of samples to process per epoch: ', num_elements)
print()
start_time_train = time.time()
qgan.train_qgan(gan_data, scaled, scaled, num_elements)
exec_time_train = time.time() - start_time_train
print(f'\nQWGAN training completed. Training time: --- {exec_time_train/3600:.02f} hours ---')

# code to SAVE qgan arrays in a file like emd_avg , acf etc

# resume metrics
if os.path.exists("qgan_metrics.npy"):
    prev = np.load("qgan_metrics.npy", allow_pickle=True).item()
    qgan.emd_avg = prev['emd_avg'] + qgan.emd_avg
    qgan.acf_avg = prev['acf_avg'] + qgan.acf_avg
    qgan.vol_avg = prev['vol_avg'] + qgan.vol_avg
    qgan.lev_avg = prev['lev_avg'] + qgan.lev_avg

# after resumed training
qgan_metrics = {
    'emd_avg': qgan.emd_avg,
    'acf_avg': qgan.acf_avg,
    'vol_avg': qgan.vol_avg,
    'lev_avg': qgan.lev_avg
}
np.save('qgan_metrics.npy', qgan_metrics)
print("QGAN metrics saved to qgan_metrics.npy")

critic_loss = tf.squeeze(qgan.critic_loss_avg, axis=(1,2)).numpy()
generator_loss = np.array(qgan.generator_loss_avg)

window = 50
generator_ma = np.convolve(generator_loss, np.ones(window)/window, mode='valid')
critic_ma = np.convolve(critic_loss, np.ones(window)/window, mode='valid')

# plot the graphs side-by-side
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))

# plot the critic loss moving average as a line
axes[0].plot(range(window-1, len(critic_loss)), critic_ma, label='Average Critic Loss', color='blue')
# plot the critic loss
axes[0].plot(critic_loss, color='black', alpha=0.2)

# plot the generator loss moving average as a line
axes[0].plot(range(window-1, len(generator_loss)), generator_ma, label='Average Generator Loss', color='orange')
# plot the generator loss
axes[0].plot(generator_loss, color='black', alpha=0.2)


axes[0].set_ylabel('Loss')
axes[0].legend()
axes[0].grid()

emd_avg = np.array(qgan.emd_avg)
emd_ma = np.convolve(emd_avg, np.ones(window)/window, mode='valid')

axes[1].plot(range(window-1, len(emd_avg)), emd_ma, label='EMD', color='red')
axes[1].plot(emd_avg, color='red', linewidth=0.5, alpha=0.5)

axes[1].set_ylabel('EMD')
axes[1].legend()
axes[1].grid()

# Adjusting the spacing between subplots
plt.tight_layout()
plt.savefig("plots/training_history (QWGAN_iio).svg", format="svg")
plt.close()
